# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Development Environment

**IMPORTANT: This is a Windows machine.** Use Windows-appropriate commands:
- Use backslashes `\` for paths (or forward slashes in Git Bash)
- Activate venv with: `venv\Scripts\activate` (Command Prompt) or `source venv/Scripts/activate` (Git Bash)
- Use Windows path format: `C:\Users\...`

## Project Overview

GermanNouns is a machine learning project that predicts the gender article (die/der/das) for German nouns based on character n-gram features. The project uses decision trees and random forests trained on the morphological patterns of German words.

Key features:
- Character n-gram feature extraction (1-grams through 5-grams)
- Chi-square feature selection for statistical significance
- Decision tree with cost complexity pruning
- Random forest with hyperparameter tuning
- Web interface for interactive predictions

This project was refactored following the lantbot package structure pattern to eliminate code duplication and improve maintainability.

## Package Structure

The project is organized as an installable Python package:

```
germannouns/
├── src/
│   └── germannouns/            # Installable package
│       ├── __init__.py         # Package version, exports
│       ├── config.py           # SINGLE source of truth for all config
│       ├── data/
│       │   ├── __init__.py
│       │   ├── loader.py       # Load and clean nouns.csv
│       │   └── feature_engineering.py  # N-grams, chi-square, feature selection
│       ├── model/
│       │   ├── __init__.py
│       │   ├── trainer.py      # Model training pipeline
│       │   └── predictor.py    # Gender prediction
│       └── utils/
│           ├── __init__.py
│           └── core.py         # Shared: create_ngram, calculate_p_value, etc.
├── scripts/                    # Thin CLI entry points
│   ├── train.py                # → germannouns.model.trainer
│   └── predict.py              # → germannouns.model.predictor
├── web/                        # Flask web demo (standalone)
│   ├── app.py                  # Flask app
│   ├── templates/index.html
│   ├── static/
│   │   ├── styles.css
│   │   └── script.js
│   └── requirements.txt
├── notebooks/                  # Exploratory analysis and research
├── tests/                      # Comprehensive test suite
│   ├── conftest.py             # Pytest fixtures
│   ├── test_core.py            # Core utilities tests
│   ├── test_data.py            # Data pipeline tests
│   └── test_model.py           # Model tests
├── nouns.csv                   # Training data (~200k German nouns)
├── decision_tree_ccp_model.pkl # Trained model (generated by training)
├── pyproject.toml              # Modern Python packaging
├── requirements.txt            # Dependencies
├── .gitignore
├── README.md
└── CLAUDE.md                   # This file
```

## Setup

### Install as Editable Package

```bash
# Create and activate virtual environment
python -m venv venv
source venv/Scripts/activate  # Git Bash
# or: venv\Scripts\activate   # Command Prompt

# Install the package in editable mode
pip install -e .

# Or install with dev dependencies
pip install -e ".[dev]"
```

### Environment Variables

Create a `.env` file in the project root (for syllable extraction research):

```
OPENAI_API_KEY=your_openai_api_key_here
```

## Running the Pipeline

### Train the Model

```bash
# Using thin CLI wrapper
python scripts/train.py

# Or using entry point (after pip install)
germannouns-train
```

This will:
1. Load and clean nouns.csv
2. Generate n-grams (1-grams through 5-grams)
3. Run chi-square feature selection
4. Train multiple models:
   - Decision tree with cost complexity pruning (recommended)
   - Decision tree with grid search
   - Random forest with random search
   - Ensemble model
5. Save best model to `decision_tree_ccp_model.pkl`

### Make Predictions

```bash
# Single prediction
python scripts/predict.py Haus
# Output: das Haus

# Interactive mode
python scripts/predict.py
# Enter noun: Katze
# → die Katze

# Using entry point
germannouns-predict Haus
```

### Run Web App

```bash
cd web
python app.py
# Visit http://localhost:5000
```

### Run Tests

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_core.py -v

# Run with coverage
pytest tests/ --cov=germannouns --cov-report=html
```

## Coding Principles

**MODULES SHOULD BE SHORT AND PRIORITIZE EXISTING UTILS. BEFORE WRITING NEW CODE, CHECK `src/germannouns/utils/` AND `src/germannouns/config.py` FOR EXISTING FUNCTIONALITY.**

**Package-Based Architecture**
- All logic lives in `src/germannouns/` as proper Python modules
- Scripts in `scripts/` are thin wrappers that import from the package
- Single source of truth: `src/germannouns/config.py` for all configuration
- Shared utilities: `src/germannouns/utils/` for common functions

**Separation of Concerns**
- Each module has ONE clear responsibility
- `data/` handles data loading and feature engineering
- `model/` handles training and prediction
- `utils/` contains shared helpers used across modules
- Don't mix concerns

**Fail Fast and Explicitly**
- Validate inputs at the start of functions
- Raise clear exceptions with actionable error messages
- Check for required files early (not when first used)
- Example: `if not api_key: raise ValueError("OPENAI_API_KEY not found in environment")`

**Import Pattern**
```python
# In src/germannouns/model/trainer.py
from germannouns.config import CCP_ALPHA, RANDOM_STATE
from germannouns.utils.core import create_ngram

# In scripts/train.py (thin wrapper)
from germannouns.model.trainer import main

if __name__ == "__main__":
    main()
```

**Simplicity & Iteration**
- Keep scripts simple and linear
- Prioritize code that's easy to modify and experiment with
- Avoid over-engineering

## Configuration

All configuration lives in `src/germannouns/config.py`:

- **Paths**: `NOUNS_CSV`, `MODEL_PICKLE`
- **Gender**: `GENDERS`, `GENDER_MAPPING`
- **N-grams**: `NGRAM_RANGE`, `START_TOKEN`, `END_TOKEN`, `NGRAM_COLUMN_NAMES`
- **Feature selection**: `P_VALUE_THRESHOLD`, `MIN_NGRAM_COUNT`, `FILTER_END_TOKEN_ONLY`
- **Model training**: `TRAIN_TEST_SPLIT`, `VALIDATION_SPLIT`, `RANDOM_STATE`, `CCP_ALPHA`
- **Grid search**: `GRID_SEARCH_PARAMS`, `RF_GRID_SEARCH_PARAMS`
- **Parallel**: `N_JOBS`

## Key Technical Decisions

1. **Character n-grams (1-5)** - Captures morphological patterns in German word endings
2. **Start/end tokens** - `<S>` and `<E>` mark word boundaries (important for gender patterns)
3. **Chi-square feature selection** - Filters to ~1000 most significant n-grams
4. **End-token filtering** - Only keeps n-grams containing `<E>` (word endings are most predictive)
5. **Cost complexity pruning** - Post-pruning with alpha=0.0001 (visualized via plot)
6. **Decision tree over random forest** - Simpler, faster, similar accuracy (~73%)
7. **Pickle persistence** - Saves both model and feature list for prediction

## Data

The `nouns.csv` file contains ~200,000 German nouns with their grammatical gender:
- **lemma**: The noun (e.g., "Haus", "Katze", "Mann")
- **genus**: Gender code ('f' = feminine, 'm' = masculine, 'n' = neuter)

Gender distribution:
- Feminine (~40%)
- Masculine (~30%)
- Neuter (~30%)

## Model Performance

With the refactored code, the model achieves:
- **Training accuracy**: ~75%
- **Test accuracy**: ~73%
- **Tree depth**: ~40 nodes

This is similar to the original implementation, confirming the refactoring preserved functionality.

## Testing

The test suite includes 40+ tests covering:
- **Core utilities**: N-gram generation, chi-square testing
- **Data pipeline**: Loading, cleaning, feature engineering
- **Model**: Training, prediction, persistence

Run tests before making changes to ensure nothing breaks.

## What Was Refactored

### Problems Addressed

1. **Code duplication**: germannouns.py and deutschnouns.py were 95% duplicated
2. **Security issue**: Exposed OpenAI API key in syllable-breakdown.py (REVOKED)
3. **Poor structure**: Monolithic main() functions, no separation of concerns
4. **No configuration management**: Magic numbers scattered throughout
5. **Missing documentation**: No docstrings or type hints
6. **Testing gaps**: Only 12 lines of tests

### Solutions Implemented

1. **Package structure**: Proper Python package with `pyproject.toml`
2. **Configuration module**: All constants in `config.py`
3. **Modular design**: Separate modules for data, model, utils
4. **Type hints & docstrings**: All functions documented
5. **Comprehensive tests**: 40+ tests with fixtures
6. **Error handling**: Validation, clear error messages
7. **CLI & web interfaces**: Both functional and user-friendly

## Deployment

The project was previously deployed on Heroku. The deployment files (Procfile, runtime.txt) should still work with the updated structure:

```
# Procfile
web: gunicorn web.app:app

# runtime.txt
python-3.11.0
```

## Future Improvements

Potential enhancements (not implemented):
- [ ] Use German language embeddings (word2vec, fastText)
- [ ] Add compound noun decomposition
- [ ] Train on morphological features (prefixes, suffixes)
- [ ] Support plural forms
- [ ] API rate limiting for web app
- [ ] Containerize with Docker
